---
title: "Delineating Watersheds, Grabbing Data"
author: "Katie Willi"
date: "2024-12-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# To install StreamCatTools:
# library(remotes)
# install_github("USEPA/StreamCatTools", build_vignettes=FALSE)

# To install climateR:
# library(remotes)
# remotes::install_github("mikejohnson51/AOI") # suggested!
# remotes::install_github("mikejohnson51/climateR")

packages <- c('tidyverse',
              'sf',
              'terra',
              'elevatr', 
              'dataRetrieval',
              'nhdplusTools',
              'StreamCatTools',
              'tmap',
              'climateR',
              'data.table',
              'mapview',
              'here')

# this package loader avoids unloading and reloading packages 
package_loader <- function(x) {
  if (!requireNamespace(x, quietly = TRUE)) {
    install.packages(x)
  }
  require(x, character.only = TRUE)
}

lapply(packages, package_loader)
```

## Grab all USGS gages

Using the dataRetrieval package in R, locate all USGS stream gages under 1500 square kilometers that measure discharge.

```{r}
states_oi <- c("Colorado", "Wyoming", "Utah", "Kansas")

us_sf_object <- tigris::states() %>% 
  filter(NAME %in% states_oi) 

# Get a list of NWIS sites for all of the states
  # what about the new NGWOS sites?
  # what about the state specific sites?
nwis_sites_by_state <- map(us_sf_object$STUSPS, 
                           ~{
                             discharge_sites <- whatNWISsites(stateCd = .x, parameterCd = "00060") %>% 
                               filter(site_tp_cd == 'ST') 
                             
                             # Only use gages under 1500 square kilometers (as defined by the USGS):
                             small_enough <- readNWISsite(discharge_sites$site_no) %>%  
                               mutate(drain_area_km = drain_area_va *  2.58999) %>%  
                               filter(drain_area_km <= 1500) %>%
                               # For future tracking with the NLDI:
                               mutate(site_pretty=paste0("USGS-",site_no),
                                      STUSPS = .x)
                             
                             return(small_enough)
                           }
) 

nwis_sites <- bind_rows(nwis_sites_by_state) %>%
  st_as_sf(coords = c("dec_long_va", "dec_lat_va"), crs = 4269)

mapview(nwis_sites)
```

## Subset to gages with data since 1980 with at least 30 years of data

This is a Katie choice that hasn't been run by MR of SK. But! To reduce our gages 
to only those with data during the time period we want (... which, I personally 
don't know what that is - you will need to confirm this with MR and SK) will reduce 
the number of watersheds you'll have to manually verify substantially. So, here 
I am filtering our gages to only sites that have at least 30 years worth of daily 
data starting in 1980. Meaning, if a gage only measured discharge before 1980, 
we are removing them as candidate gages. We are also removing any gages that don't 
have at least 30 years of data since 1980.

```{r}
gage_meta <- dataRetrieval::whatNWISdata(siteNumber = nwis_sites$site_no, parameterCd = "00060")
# Why is this list longer than nwis_sites?

# Scrape the USGS water parameter code table 
tables <- rvest::read_html('https://help.waterdata.usgs.gov/parameter_cd?group_cd=%') %>% # fetch the webpage
  rvest::html_nodes('table') %>% # extract the table elements from the HTML
  rvest::html_table() # convert the HTML tables into R data frames

# create parm_cd (parameter code?) column in the USGS water parameter code table
pcodes <- tables[[1]] %>% 
  janitor::clean_names() %>% 
  dplyr::mutate(parm_cd = stringr::str_pad(as.character(parameter_code), 5, pad = "0"))

inventory <- gage_meta %>%
  dplyr::left_join(pcodes,by="parm_cd") %>%
  dplyr::select(c(site_name = station_nm,
                  site_no,
                  data_type_cd,
                  site_type_cd = site_tp_cd,
                  n_obs = count_nu,
                  begin_date,
                  end_date,
                  parameter = parameter_name_description,
                  code = parm_cd))

# Scrape the USGS NWIS Site types tables 
site_url <- 'https://maps.waterdata.usgs.gov/mapper/help/sitetype.html'

table <- rvest::read_html(site_url) %>%
  rvest::html_nodes('table') %>%
  rvest::html_table() 

table <- rbind(table[[1]],table[[2]],table[[3]],table[[4]],table[[5]]) %>%
  dplyr::select(site_type_cd = 1,
                site_type = 2)

inventory <- left_join(inventory, table, by = 'site_type_cd') %>%
  mutate(data_type = case_when(data_type_cd == "dv" ~ "Daily",
                               data_type_cd == "uv" ~ "Unit",
                               data_type_cd == "qw" ~ "Water Quality",
                               data_type_cd == "gw" ~ "Groundwater Levels",
                               data_type_cd == "iv" ~ "Unit",
                               data_type_cd == "sv" ~ "Site Visits",
                               data_type_cd == "pk" ~ "Peak Measurements",
                               data_type_cd == "ad" ~ "USGS Annual Water Data Report",
                               data_type_cd == "aw" ~ "Active Groundwater Level Network",
                               data_type_cd == "id" ~ "Historic Instantaneous"))

new_data_gages <- inventory %>%
  filter(year(end_date) >= "1980",
         data_type == "Daily") %>%
  # and only sites with at least 30 years worth of data -ish
  filter(n_obs >= 365*30)

# Only keep gages with data during or after 1980:
nwis_sites <- nwis_sites %>% 
  filter(site_no %in% new_data_gages$site_no)

# Down from 1000 to 221 after this
```

Grab CO DWR gages... not currently running this but here's the code for when you want to begin including those gages as well.

```{r, eval = FALSE}
# # This is where you tack on the other state specific gages
# cdwr_sites <- httr::GET(url = "https://dwr.state.co.us/Rest/GET/api/v2/surfacewater/surfacewaterstations/?format=json&fields=stationNum%2Cabbrev%2CusgsSiteId%2CstationName%2CutmX%2CutmY%2Clatitude%2Clongitude%2CstartDate%2CendDate%2CmeasUnit") %>%
#   httr::content(., as = "text", encoding = "UTF-8") %>%
#   jsonlite::fromJSON() %>%
#   .[["ResultList"]] %>%
#   filter(year(endDate) >= "1980") %>%
#   filter(is.na(usgsSiteId)) %>%
#   filter(!is.na(longitude) & !is.na(latitude)) %>%
#   st_as_sf(coords = c("longitude", "latitude"), crs = 4269) %>%
#   # Obnoxiously station type cannot be accessed on API only GUI
#   filter(abbrev %in% c(read_csv("data/cdwr.csv") %>%.$Abbrev)) %>%
#   select(site_no = abbrev,
#          station_nm = stationName)
# 
# # Bind CO DWR and USGS sites together
# nwis_sites <- nwis_sites %>% bind_rows(cdwr_sites)}
```

TODO:
Grab WY DWR gages
Grab UT DWR gages
Grab KS DWR gages

## Delineate stream gage watersheds

For this analysis, we are relying on NHDPlus Version 2. We are able to delineate 
a stream gage's upstream contributing area (i.e., its watershed) by leveraging 
the NHD's network index that, for every stream feature in the NHD, identifies all 
other stream features upstream of it. So, our first task is to find out which NHD 
stream feature each gage is associated with. All stream features are given a 
unique ID, called a comid. Every stream feature also has an associated "catchment", 
or direct contributing area, with the same comid. So here, we are identifying the 
comid for the stream feature each gage is associated with. 
**DANGER: YOU WILL NEED TO CONFIRM THE APPROPRIATE FLOWLINE IS SELECTED FOR EVERY GAGE. THERE IS NO WAY TO CONFIRM THEY ARE RIGHT WITHOUT EYEBALLING THEM!**

```{r}
nwis_sites$comid <- NA # attempt 'get_nldi_feature()' first
nwis_sites$comid_coords <- NA # if that doesn't work for all gages, do 'discover_nhdplus_id()'

# TODO: convert these for loops to maps/mutates
# first try to get comid using nldi ("verified" correct comid - or at least what USGS says it is)
for(i in 1:nrow(nwis_sites)){
  try(nwis_sites$comid[i] <- get_nldi_feature(list("featureSource" = "nwissite", featureID = nwis_sites[i,]$site_pretty))$comid, silent = T)
}

# get the comid using the weirdos' coordinates instead of their gage name
for(i in 1:nrow(nwis_sites)){
  nwis_sites$comid_coords[i] <- discover_nhdplus_id(nwis_sites[i,])
}

# TODO: make sure that the data folder is properly set up for symlinks
# back it up if you want: !!!
# saveRDS(nwis_sites, 'data/nwis_gages_comid.RDS')

# Ones where the USGS says they fall on a comid they don't technically fall on. For these, it is highly likely that you will need to go 
# one-by-one to identify which COMID is actually appropriate to attribute to them: 
weirdos <- nwis_sites %>% filter(comid_coords != comid)

mapview(weirdos) + 
  mapview(get_nhdplus(comid = weirdos$comid), color = "blue", layer.name = "By NLDI") + 
  mapview(get_nhdplus(comid = weirdos$comid_coords), color = "red", layer.name = "By coordinates")

nwis_sites <- nwis_sites %>%
  # UPDATE THIS SECTION - This is where the exercise above will dictate which of the comids is most appropriate to use for each gage.
  # For now, just removing the weirdos.
  mutate(comid_new = ifelse(is.na(comid), comid_coords, comid)) %>%
  filter(comid_coords == comid) %>%
  select(STUSPS, site_no, station_nm, comid = comid_new) %>%
  mutate(comid = as.numeric(comid)) 
  
```

##### Delineate each gage's watershed

Now that we have a list of our gages and their associated NHDPlus V2 stream 
features, we can use the NHD indexing to "crawl" upstream of each gage's flowline, 
then grab each flowline's catchment, and lastly dissolve those catchments into a 
single polygon the represents the gage's upstream contributing area (i.e., its watershed):

```{r}
# load in the NHD as a table. This table lists all COMIDs in CONUS and allows you to "navigate" the NHD.
nhd <- read_csv(here("data", "nhd_flow_network.csv"))

# function to delineate each gage's watershed:
# - gut check with better termination 
watershed_delineator <- function(site_list){
  
  # filter our master list to just the gage we are iterating over
  site <- nwis_sites %>%
    filter(site_no == site_list)
  
  # use get_UT to list all comids that are upstream of our gage using the comid the
  # gage falls on:
  upstream <- nhdplusTools::get_UT(nhd, site$comid)
  
  # grab all the catchments associated with the upstream comids:
  nhd_catch <- nhdplusTools::get_nhdplus(comid = upstream,
                                   realization = 'catchment',
                                   t_srs = 4269) %>%
    # remove dupes (precautionary step, not likely necessary)
    dplyr::distinct(featureid, .keep_all=TRUE) %>%
    # "dissolve" all the catchments into a single polygon
    dplyr::summarize() %>% # this makes the watershed
    # remove weird hole by-products that exist if the catchment boundaries don't
    # line up perfectly:
    nngeo::st_remove_holes() %>%
    # tack on the state, site name, and comid to the watershed
    dplyr::mutate(state = site$STUSPS,
                  site_no = site$site_no,
                  comid = site$comid)
  # back it up:
  saveRDS(nhd_catch, here("data", "watersheds", paste0(site$STUSPS, "_", site$site_no, ".RDS")))
  # saveRDS(nhd_catch, paste0("data/watersheds/", site$site_no, ".RDS"))
  
  print(paste0(site$station_nm, " delineated!"))
  
  # return the delineated watershed
  return(nhd_catch)
  
}

# Create a vector of nwis sites to iterate over
watersheds <- nwis_sites$site_no %>%
  #... then delineate each site's watershed:
  map(~watershed_delineator(.)) %>%
  bind_rows()

mapview::mapview(watersheds)
```

## Grab explanatory variables

Link up gages with streamcat variables. StreamCat watershed statistics are available 
for every stream feature in the NHDPlusV2. StreamCat uses the comid as the identifier 
so we can link up information that way. A complete list of available variables you 
can pull is found in the vars table below. I am NOT pulling in all the available 
info that you can because there is so much!

```{r}
# Grab a list of all available streamcat variables:
download.file("https://java.epa.gov/StreamCAT/metrics/variable_info.csv",
              destfile = paste0(getwd(), "/data/StreamCatVars.csv"))

# This table describes all the available variables in streamcat. Look here
# if you want to explore other vars, or the descriptions of the ones I've
# selected here:
vars <- read_csv("data/StreamCatVars.csv")

# This is what's available on StreamCat related to lithology. Likely not identical to what
# was used by Abby but hopefully good swap:
lithology_vars <- c("pctalkintruvol", "pctwater",
                    "pctsilicic", "pctsallake",    
                    "pctnoncarbresid", "pcthydric",      
                    "pctglactilloam", "pctglactilcrs",  
                    "pctglactilclay", "pctglaclakefine",
                    "pctglaclakecrs", "pctextruvol",   
                    "pcteolfine", "pcteolcrs",      
                    "pctcolluvsed", "pctcoastcrs",    
                    "pctcarbresid")
                    # "pctalluvocoast" is giving me issues

# Urban cover (add all together to get percent of total urban cover): 
# Many available years. Which do we want to use? For now using 2011:
urban_cover <- c("pcturbop2019", "pcturbmd2019", "pcturblo2019", "pcturbhi2019")

# PRISM mean precip for 1981-2010 OR 1991-2020
prism_precip <- c("precip8110", "precip9120")

# These are all the variables Fred was interested in for describing flow
# in his work. Likely a good starting point for our needs, too. 
fred_vars <- c("canaldens", 
               # BFI
               "bfi", 
               #NLCD 2019
               "pctow2019", "pctice2019", "pcturbop2019", "pcturblo2019", "pcturbmd2019", "pcturbhi2019",
               "pctbl2019", "pctdecid2019", "pctconif2019", "pctmxfst2019", "pctshrb2019", "pctgrs2019",
               "pcthay2019", "pctcrop2019", "pctwdwet2019", "pcthbwet2019",
               # Dam Info
              "damdens", "damnidstor", "damnrmstor",
               # Elevation
               "elev", 
               # Impervious Surfaces across a bunch of years:
               "pctimp2006", "pctimp2008", "pctimp2011", "pctimp2001",
               "pctimp2013", "pctimp2019", "pctimp2016", "pctimp2004",
               # PRISM 1991-2020
               "precip9120", "tmax9120", "tmean9120", "tmin9120",
               # STATSGO variables:
               "clay", "sand", "wtdep", "om", "perm", "rckdep") # "silt" is giving me issues

streamcat_vars <- StreamCatTools::sc_get_data(metric = paste(c(lithology_vars, urban_cover, prism_precip, fred_vars), collapse = ","),
                                      aoi = 'watershed', 
                                      comid = nwis_sites$comid) %>%
  # remove variables we don't particularly care about that get returned:
  select(-contains("AREASQKM"))

# combine the watershed polygons with the streamcat data
watersheds_streamcat <- watersheds %>%
  left_join(., streamcat_vars, by = "comid") %>%
  mutate(pcturb2019ws = pcturbhi2019ws + pcturbmd2019ws + pcturblo2019ws + pcturbop2019ws)
```

## Find reference quality gages:

Using the StreamCat variables, we can drop any gages who have characteristics 
that make them unsuitable as reference gages. For example, we can remove any 
gages whose watersheds have \>= 10% urban landcover and watersheds that have 
dam storage densities larger than 100 megaliters/square kilometer:

```{r}
ref_watersheds <- watersheds_streamcat %>%
  filter(pcturb2019ws < 10) %>%
  filter(damnidstorws < 100000)
# REMOVE THIS STEP ONCE "GOOD" WATERSHEDS HAVE BEEN IDENTIFIED:
# But for now, this is a quick and lazy way of getting rid of watersheds
# we know were delineated incorrectly. They were delineated incorrectly
# because their attributed comids are wrong. (See comment about the weirdos
# object above):
ref_watersheds <- ref_watersheds %>% mutate(area = sf::st_area(ref_watersheds)) %>%
  # remove any watersheds larger than 1600 km (for some conservative wiggle 
  # with projection differences)
  filter(as.numeric(area) <= 1.6e+9)
```

And, we can drop any gages whose watersheds contain a transbasin diversion. We 
identify watersheds that have a transbasin diversion by grabbing NHD HR flowlines 
features that intersect the watershed. We are using NHD HR instead of NHDPlusV2 
because the NHDHR has flowline features that are identified as being canals, 
ditches, etc. With that info, we identify watersheds where any of those "unnatural" 
features cross over the watershed boundary. If a canal/ditch crosses over a 
watershed boundary, that means that water is being moved in or out of the 
watershed unnaturally.

This function takes a long time.
```{r}
fetch_flowlines <- function(site_list){
  
  site <- watersheds %>%
    filter(site_no == site_list)
  
  # open the nhd_hr - which contains a bunch of layers
  nhd_hr <- arcgislayers::arc_open("https://hydro.nationalmap.gov/arcgis/rest/services/NHDPlus_HR/MapServer")
  
  # arcgislayers::list_items(nhd_hr)
  
  nhd_hr_flowlines <- arcgislayers::get_layer(nhd_hr, 3)
  
  # use bbox to return associated flowlines
  geospatial_aoi <- site %>% 
    # add a buffer around the watershed for visualization later on
    st_buffer(1000) %>% 
    # Convert sf object to sfc object (required for downloading from the map server)
    st_as_sfc(.)
  
  nhd_flowlines <- vector("list", length = length(geospatial_aoi))
  
  tryCatch({
    nhd_flowlines <- arcgislayers::arc_select(nhd_hr_flowlines,
                                              # where = query,
                                              filter_geom = geospatial_aoi,
                                              crs = st_crs(geospatial_aoi)) %>% 
      st_make_valid()},
    error = function(e){
      cat("Index ", i, " from input data failed.")
    }) 
  
  nhd_flowlines <- nhd_flowlines %>% 
    keep(~!is.null(.))
  
  try(nhd_flowlines <- nhd_flowlines %>% 
        dplyr::bind_rows() %>%
        dplyr::distinct() %>%
        mutate(#natural = ifelse(ftype == 460, T, F),
          flowline_type = case_when(ftype == 460 ~ "natural",
                                    ftype == 558 ~ "artificial path",
                                    ftype == 468 ~ "drainageway",
                                    ftype == 336 ~ "canal ditch",
                                    ftype == 566 ~ "coastline",
                                    ftype == 334 ~ "connector",
                                    ftype == 428 ~ "pipeline",
                                    ftype == 420 ~ "underground conduit",
                                    .default = "unnatural")), 
      silent = TRUE)
  
  saveRDS(nhd_flowlines, here("data", "flowlines", paste0(site$state, "_", site$site_no,".RDS")))
  # saveRDS(nhd_flowlines, paste0("data/flowlines/", site_list, ".RDS"))
  
  print(paste0(site_list, " done!"))
  
  return(nhd_flowlines)
  
}

all_flowlines <- ref_watersheds$site_no %>%
  map(~fetch_flowlines(.)) %>%
  bind_rows() %>%
  distinct()
```

```{r}
# in fetchNHD_flowlines, we have categorized each flowline as being natural or unnatural.
# So, we can subset the flowlines to just the unnatural ones. 
all_flowlines_unnatural <- all_flowlines %>%
  filter(flowline_type != "natural") 

transbasin_finder <- function(site_list){ 
  
  # filter our master list to just the gage's watershed we are iterating over
  site <- ref_watersheds %>%
    filter(site_no == site_list)
  
  flowlines <- read_rds(here("data", "flowlines", paste0(site$state, "_", site$site_no, ".RDS"))) 
  
  # if there are no flow lines return NULL
  if (length(flowlines) == 0) {
    print(paste(site$site_no, "has an empty list"))
    return(NULL)
  }
  
  flowlines_unnatural <- flowlines %>%
    filter(flowline_type != "natural") 
  
  # For linestring transformation step to work, need the watershed to be a polygon object:
  if (st_geometry_type(site) != "POLYGON") {
    # If not, cast to a Polygon... which will "explode" it into multiple. 
    # This is a rare thing... I think...
    site <- st_cast(site, "POLYGON") 
  }
  
  polyline <-  site %>% st_cast("LINESTRING")
  
  crossovers <- flowlines_unnatural %>%
    .[polyline,] %>%
    nrow()
  
  # Some watersheds are multipolygons and therefore need to be put back together here:
  site <- site %>% group_by(site_no, comid) %>% summarize() %>%
    mutate(transbasin = ifelse(crossovers > 0, "TRANSBASIN DIVERSION", "NATURAL")) 
  
  # Extract the bounding box of the site_data
  bbox_site <- st_bbox(site)
  
  # Create the ggplot map, zoomed to the bounding box of site_data
  gg_map <- ggplot() +
    # Plot the site data
    geom_sf(data = site, color = "black", fill = "white", size = 1) + 
    # Plot the site data
    geom_sf(data = filter(nwis_sites, site_no == site$site_no), color = "lightblue", size = 5.5) + 
    # Plot the natural flowlines in blue
    geom_sf(data = flowlines, color = "blue", size = 0.5) + 
    # Plot the unnatural flowlines in red
    geom_sf(data = flowlines_unnatural, color = "red", size = 2) + 
    # Set the xlim and ylim based on the bounding box of site_data
    xlim(bbox_site["xmin"], bbox_site["xmax"]) +
    ylim(bbox_site["ymin"], bbox_site["ymax"]) +
    coord_sf() + 
    theme_void() +
    labs(title = paste0(site$site_no, " ", site$transbasin)) +  
    theme(
      plot.title = element_text(size = 14, hjust = 0.5),  
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank()
    )
  
  # Save the map as an image
  ggsave(gg_map, filename = here("data", "transbasin_confirm", paste0(site$transbasin, "_", site$state, "_", site$site_no, ".png")))
  
  print(paste0(site$site_no, " was successful."))
  return(site)
  
}

watersheds_div <- ref_watersheds$site_no %>%
  map(~transbasin_finder(.)) %>%
  compact() %>% 
  bind_rows() %>%
  st_make_valid()
```

Reduce our reference gages to only gages without a transbasin diversion:

```{r}
ref_watersheds <- watersheds_div %>%
  filter(transbasin == "NATURAL")

# only 35 reference watersheds???
```

### Grab other variables not found in stream cat:

#### Dominant watershed aspect

Dominant aspect requires a bit of a wonky workflow - I'm not quite sure if there's 
an easier approach than what I'm presenting here. I grab raw elevation DEMs, get 
the aspect (in degrees) for each grid cell, then convert those raw aspects 
(in degrees) into categorical N, E, S, W cardinal directions as displayed in 
this image below:

![](data/compassrose.jpg)

```{r}
aspect_finder <- function(site_list){
  
  # create numerical representation for each cardinal aspect:
  aspect_lookup <- tibble(val = c(1, 2, 3, 4),
                          aspect = c("North", "East", "South", "West"))
  
  # filter our master list to just the gage's watershed we are iterating over
  site <- ref_watersheds %>%
    filter(site_no == site_list)
  
  # grab elevation data
  elev <- elevatr::get_elev_raster(summarize(site), z = 12, clip = "locations") %>% # zoom of 12 is close-ish(?) to 30 meters... JD should confirm!
    terra::rast() %>% 
    # clip to extent of the watershed
    terra::mask(., site, touches = FALSE)
  
  # calculate aspect from the masked elevation
  aspect <- terra::terrain(elev, v = 'aspect')
  
  # convert aspect values to cardinal directions
  convert_to_direction <- function(aspect) {
    direction <- rep(NA, length(aspect))
    direction[aspect >= 0 & aspect <= 45 | aspect > 315 & aspect <= 360] <- 1  # North
    direction[aspect > 45 & aspect <= 135] <- 2  # East
    direction[aspect > 135 & aspect <= 225] <- 3  # South
    direction[aspect > 225 & aspect <= 315] <- 4  # West
    return(direction)
  }
  
  # apply the conversion directly to the raster values
  aspect_cardinal_raster <- terra::app(aspect, fun = convert_to_direction) 
  
  #Map showing what this aspect layer looks like geospatially:
  plot(aspect_cardinal_raster)
  
  # Calculate the mode (dom aspect) in each watershed
  dominant_aspect <- as.data.table(aspect_cardinal_raster) %>%
    rename(val = lyr.1) %>%
    group_by(val) %>%
    summarize(count = n()) %>%
    filter(count == max(count)) %>%
    left_join(aspect_lookup, by = "val") %>%
    mutate(site_no = site$site_no)
  
}

watershed_aspects <- ref_watersheds$site_no %>%
  map(~aspect_finder(.)) %>%
  bind_rows()
```

#### GridMET climate data

gridMET is DAILY gridded climate data. I am pulling in daily data for all grid 
cells that overlap each watershed for days in 2001-2020. Namely, I'm downloading 
max temperature, min temperature, PET, and precipitation. Then, I'm averaging 
that data across the watershed to get a single, average value for the watershed. 
See the function `get_climate_historic()` if you want to see how the raw 
gridMET data is pulled in.

```{r}

get_climate_historic <- function(sf,
                                 col_name,
                                 start = "1979-01-01",
                                 end = "2023-12-31",
                                 vars = c("tmmx", "tmmn", "pr", "pet")) {
  
  sf <- sf %>%
    dplyr::rename("join_index" = {{col_name}})
  
  all_climate_data <- vector("list", length = nrow(sf))
  
  if(any(unique(sf::st_geometry_type(sf)) %in% c("POLYGON", "MULTIPOLYGON"))){
    
    for (i in 1:nrow(sf)) {
      
      aoi <- sf[i,]
      browser()
      print(paste0('Downloading GridMET for ', aoi$state, "_", aoi$join_index, "."))
      
      clim <- climateR::getGridMET(AOI = aoi,
                                   varname = vars,
                                   startDate = start,
                                   endDate = end)
      
      
      if(inherits(clim[[1]], "SpatRaster")){
        
        
        clim_crs <- crs(clim[[1]])
        
        if(st_crs(clim[[1]]) != st_crs(sf)){
          
          clim <- clim %>%
            purrr::map(
              # getGridMET defaults AOI to bbox - so crop / mask results to sf extent
              ~terra::crop(., st_transform(aoi, crs = clim_crs), mask = TRUE),
              crs = clim_crs)
        } else {
          
          clim <- clim %>%
            purrr::map(
              # getGridMET defaults AOI to bbox - so crop / mask results to sf extent
              ~terra::crop(., aoi, mask = TRUE),
              crs = clim_crs)
          
        }
        
        all_climate_data[[i]] <- clim %>%
          purrr::map_dfr(~ as.data.frame(., xy = TRUE)) %>%
          data.table() %>%
          pivot_longer(-(x:y),
                       names_to = "var_temp",
                       values_to = "val") %>%
          separate_wider_delim(var_temp, "_", names = c("var", "date")) %>%
          drop_na(val) %>%
          group_by(x, y, date) %>%
          pivot_wider(names_from = "var", values_from = "val") %>%
          dplyr::mutate(date = as.Date(date),
                        pet_mm = pet,
                        ppt_mm = pr,
                        tmax_C = tmmx - 273.15,
                        tmin_C = tmmn - 273.15,
                        tmean_C = (tmax_C + tmin_C)/2,
                        join_index = aoi$join_index) %>%
          dplyr::select(-c("tmmx", "tmmn", "pr", "pet"))
        
        saveRDS(all_climate_data[[i]], here("data", "climate", paste0(aoi$state, "_", aoi$join_index, ".RDS")))
        
      } else {
        
        all_climate_data[[i]] <- clim %>%
          data.table() %>%
          # names of columns include va_mode_rcp so must rename
          rename_with(~ str_split(.x, "_", n = 2) %>% map_chr(1)) %>%
          # since polygon grabbed a single grid, gridMET does not provide the coordinates
          # of the gridMET cell, so we fill in x and y with the coordinates
          # of the sf object:
          dplyr::mutate(x = sf::st_coordinates(aoi)[[1]],
                        y = sf::st_coordinates(aoi)[[2]]) %>%
          # Then do all other cleaning steps done for polygon sf objects:
          dplyr::mutate(date = as.Date(date),
                        pet_mm = pet,
                        ppt_mm = pr,
                        tmax_C = tmmx - 273.15,
                        tmin_C = tmmn - 273.15,
                        tmean_C = (tmax_C + tmin_C)/2,
                        join_index = aoi$join_index) %>%
          dplyr::select(-c("tmmx", "tmmn", "pr", "pet"))
        
        saveRDS(all_climate_data[[i]], here("data", "climate", paste0(aoi$state, "_", aoi$join_index, ".RDS")))
        
      }
    }
    
    all_climate_data <- all_climate_data %>%
      bind_rows()
    
    # Rename the join_index column
    colnames(all_climate_data)[colnames(all_climate_data) == "join_index"] <- {{col_name}}
    
    return(all_climate_data)
    
  } else {
    stop("Your sf feature is neither a polygon nor point feature, or it needs to be made valid.")
    }
  
}

watershed_climate <- get_climate_historic(sf = ref_watersheds,
                                          col_name = "site_no", 
                                          # Snow persistence start
                                          start = "2001-01-01",
                                          # Snow persistence end
                                          end = "2020-12-31",
                                          vars = c("pet", "pr", "tmmn", "tmmx")) %>%
  group_by(site_no) %>% 
  summarize(pet_mm_2001_2020 = mean(pet_mm, na.rm = TRUE),
            ppt_mm_2001_2020 = mean(ppt_mm, na.rm = TRUE), 
            tmax_C_2001_2020 = mean(tmax_C, na.rm = TRUE), 
            tmin_C_2001_2020 = mean(tmin_C, na.rm = TRUE))
```
```{r}
# rename the climate data in the climate folder

```

#### Snow persistence

Here I am loading in all the snow persistence data I downloaded from: <https://www.sciencebase.gov/catalog/item/5f63790982ce38aaa23a3930>. There is 
annual snow persistence data from 2001-2020. Using the {terra} package, I get 
the area-weighted annual average snow persistence value for each watershed, 
then average each year's data together to get a single time- and area-weighted 
average for each watershed.

Q: This is not the MODIS data set?

```{r}
# sp_preview <- rast("data/snow_persistence_hammond/MOD10A2_SCI_2020.tif")
# 
# tm_shape(sp_preview) + 
#   tm_raster(palette = "viridis", title = "Snow Persistence") +
#   tm_layout(frame = FALSE)

# load all the .tif files into a list
tif_files <- list.files(here("data", "snow_persistence_hammond"), pattern = "\\.tif$", full.names = TRUE)

# stack the snow persistence .tif files into a single raster stack
raster_stack <- rast(tif_files)

# convert the shapefile to a 'terra' object (if necessary)
polygon <- st_transform(ref_watersheds, crs(raster_stack))  # Align CRS

# convert the polygons to 'terra' vector format
polygon_terra <- vect(polygon)

# mask the raster stack to the watershed polygons
masked_stack <- mask(raster_stack, polygon_terra)

# extract mean SP across each watershed. weights = TRUE means get the area-weighted average
mean_sp <- extract(masked_stack, polygon_terra, fun = mean, weights = TRUE)

# convert the results to a data frame listing each gage's SP
watershed_sp <- as_tibble(mean_sp) %>%
  bind_cols(st_drop_geometry(ref_watersheds)) %>%
  select(-ID) %>%
  pivot_longer(cols = c(contains("MOD"))) %>%
  group_by(state, site_no, comid, transbasin) %>% 
  summarize(mean_sp_2001_2020 = mean(value))
```

get the list of watersheds to start manually verifying.
add state specific gages
